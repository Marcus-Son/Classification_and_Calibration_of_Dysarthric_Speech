{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 디렉토리 경로\n",
    "data_dir = \"/content/gdrive/My Drive/deeplearningproject/결과\"\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "random_seed = 42\n",
    "tf.random.set_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# 데이터 로드\n",
    "batch_size = 32\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "\n",
    "dataset = image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    image_size=(img_height, img_width),\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# 클래스 비율 확인\n",
    "class_names = dataset.class_names\n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "class_counts = np.zeros(len(class_names))\n",
    "for _, labels in dataset:\n",
    "    class_counts += np.sum(labels.numpy(), axis=0)\n",
    "\n",
    "print(f\"Class distribution: {class_counts}\")\n",
    "\n",
    "# 데이터셋을 numpy array로 변환\n",
    "def dataset_to_numpy(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img_batch, label_batch in dataset:\n",
    "        images.append(img_batch.numpy())\n",
    "        labels.append(label_batch.numpy())\n",
    "    return np.concatenate(images), np.concatenate(labels)\n",
    "\n",
    "images, labels = dataset_to_numpy(dataset)\n",
    "\n",
    "# 데이터셋을 3:1:1 비율로 나누기\n",
    "train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
    "    images, labels, test_size=0.4, stratify=labels, random_state=random_seed)\n",
    "\n",
    "val_images, test_images, val_labels, test_labels = train_test_split(\n",
    "    temp_images, temp_labels, test_size=0.5, stratify=temp_labels, random_state=random_seed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 데이터 증강 생성기 정의\n",
    "datagen = ImageDataGenerator(\n",
    "    # rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    # shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    # horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# 증강된 데이터를 이용한 학습 데이터셋 생성\n",
    "def augment_dataset(images, labels, datagen, batch_size):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    for img, label in zip(images, labels):\n",
    "        img = img.reshape((1,) + img.shape)\n",
    "        aug_iter = datagen.flow(img, batch_size=1)\n",
    "        aug_img = aug_iter.next().astype(np.float32)\n",
    "        augmented_images.append(aug_img.reshape(img.shape[1:]))\n",
    "        augmented_labels.append(label)\n",
    "    return np.array(augmented_images), np.array(augmented_labels)\n",
    "\n",
    "augmented_train_images, augmented_train_labels = augment_dataset(train_images, train_labels, datagen, batch_size)\n",
    "\n",
    "# numpy array를 tf.data.Dataset으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy array를 tf.data.Dataset으로 변환\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(batch_size)\n",
    "augmented_train_dataset = tf.data.Dataset.from_tensor_slices((augmented_train_images, augmented_train_labels)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 데이터셋의 클래스 비율 확인\n",
    "def get_class_distribution(labels):\n",
    "    class_counts = np.sum(labels, axis=0)\n",
    "    return class_counts\n",
    "\n",
    "print(f\"Train class distribution: {get_class_distribution(train_labels)}\")\n",
    "print(f\"Validation class distribution: {get_class_distribution(val_labels)}\")\n",
    "print(f\"Test class distribution: {get_class_distribution(test_labels)}\")\n",
    "\n",
    "# 데이터셋에서 이미지 shape 확인\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(f\"Train Image batch shape: {images.shape}\")\n",
    "    print(f\"Train Label batch shape: {labels.shape}\")\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[np.argmax(labels[i])])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "for images, labels in val_dataset.take(1):\n",
    "    print(f\"Validation Image batch shape: {images.shape}\")\n",
    "    print(f\"Validation Label batch shape: {labels.shape}\")\n",
    "\n",
    "for images, labels in test_dataset.take(1):\n",
    "    print(f\"Test Image batch shape: {images.shape}\")\n",
    "    print(f\"Test Label batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "checkpoint_dir = \"/content/gdrive/My Drive/deeplearningproject/best_model\"\n",
    "\n",
    "# 모델 정의\n",
    "def create_baseline1_model():\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=(28, 28, 3)),\n",
    "        layers.Conv2D(8, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.AveragePooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.AveragePooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.AveragePooling2D((2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# VGG 모델 정의\n",
    "def create_vgg_model():\n",
    "    model = models.Sequential([\n",
    "        layers.InputLayer(input_shape=(28, 28, 3)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# DenseNet 모델 정의\n",
    "def create_densenet_model():\n",
    "    def dense_block(x, blocks, growth_rate):\n",
    "        for _ in range(blocks):\n",
    "            x = conv_block(x, growth_rate)\n",
    "        return x\n",
    "\n",
    "    def conv_block(x, growth_rate):\n",
    "        x1 = layers.BatchNormalization()(x)\n",
    "        x1 = layers.ReLU()(x1)\n",
    "        x1 = layers.Conv2D(4 * growth_rate, (1, 1), use_bias=False)(x1)\n",
    "        x1 = layers.BatchNormalization()(x1)\n",
    "        x1 = layers.ReLU()(x1)\n",
    "        x1 = layers.Conv2D(growth_rate, (3, 3), padding='same', use_bias=False)(x1)\n",
    "        x = layers.Concatenate()([x, x1])\n",
    "        return x\n",
    "\n",
    "    def transition_block(x, reduction):\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Conv2D(int(x.shape[-1] * reduction), (1, 1), use_bias=False)(x)\n",
    "        x = layers.AveragePooling2D((2, 2), strides=2)(x)\n",
    "        return x\n",
    "\n",
    "    inputs = layers.Input(shape=(28, 28, 3))\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', use_bias=False)(inputs)\n",
    "    x = dense_block(x, blocks=4, growth_rate=12)\n",
    "    x = transition_block(x, reduction=0.5)\n",
    "    x = dense_block(x, blocks=4, growth_rate=12)\n",
    "    x = transition_block(x, reduction=0.5)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ResNet 모델 정의\n",
    "def create_resnet_model():\n",
    "    def residual_block(x, filters, kernel_size=3, stride=1, conv_shortcut=True):\n",
    "        shortcut = x\n",
    "        if conv_shortcut:\n",
    "            shortcut = layers.Conv2D(filters, (1, 1), strides=stride)(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same', strides=stride)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.add([shortcut, x])\n",
    "        x = layers.ReLU()(x)\n",
    "        return x\n",
    "\n",
    "    inputs = layers.Input(shape=(28, 28, 3))\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = residual_block(x, 128, stride=2)\n",
    "    x = residual_block(x, 128)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    x = residual_block(x, 256, stride=2)\n",
    "    x = residual_block(x, 256)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#inception 모델 정의\n",
    "def create_inception_model(input_shape=(28, 28, 3), num_classes=2):\n",
    "    base_model = InceptionV3(weights=None, include_top=False, input_shape=input_shape)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, train_dataset, val_dataset, test_dataset):\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0.00001)\n",
    "    checkpoint_path_ensemble = os.path.join(checkpoint_dir, \"best_model_ensemble.h5\")\n",
    "    checkpoint_ensemble = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_ensemble, monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=100,\n",
    "        batch_size=20,\n",
    "        callbacks=[checkpoint_ensemble,reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_path_ensemble)\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test loss: {loss}\")\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "\n",
    "    y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_true, axis=1)\n",
    "\n",
    "    report = classification_report(y_true_classes, y_pred_classes, target_names=class_names, output_dict=True)\n",
    "    return history, report, y_pred, y_true_classes\n",
    "\n",
    "def calculate_ece(y_true, y_prob, n_bins=10):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_indices = np.digitize(np.max(y_prob, axis=1), bins) - 1\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_mask = bin_indices == i\n",
    "        bin_size = np.sum(bin_mask)\n",
    "        if bin_size > 0:\n",
    "            bin_accuracy = np.mean(y_true[bin_mask] == np.argmax(y_prob[bin_mask], axis=1))\n",
    "            bin_confidence = np.mean(np.max(y_prob[bin_mask], axis=1))\n",
    "            ece += np.abs(bin_accuracy - bin_confidence) * bin_size / len(y_true)\n",
    "    return ece\n",
    "\n",
    "def calculate_oe(y_true, y_prob, n_bins=10):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_indices = np.digitize(np.max(y_prob, axis=1), bins) - 1\n",
    "    oe = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_mask = bin_indices == i\n",
    "        bin_size = np.sum(bin_mask)\n",
    "        if bin_size > 0:\n",
    "            bin_accuracy = np.mean(y_true[bin_mask] == np.argmax(y_prob[bin_mask], axis=1))\n",
    "            bin_confidence = np.mean(np.max(y_prob[bin_mask], axis=1))\n",
    "            if bin_confidence > bin_accuracy:\n",
    "                oe += (bin_confidence - bin_accuracy) * bin_size / len(y_true)\n",
    "    return oe\n",
    "\n",
    "# 모델 1 (Baseline1)\n",
    "model1 = create_baseline1_model()\n",
    "history1, report1, y_pred1, y_true_classes = train_and_evaluate_model(model1, train_dataset, val_dataset, test_dataset)\n",
    "# history1, report1, y_pred1, y_true_classes = train_and_evaluate_model(model1, augmented_train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "# 모델 2 (VGG)\n",
    "model2 = create_vgg_model()\n",
    "history2, report2, y_pred2, _ = train_and_evaluate_model(model2, train_dataset, val_dataset, test_dataset)\n",
    "# history2, report2, y_pred2, _ = train_and_evaluate_model(model2, augmented_train_images, val_dataset, test_dataset)\n",
    "\n",
    "# 모델 3 (DenseNet)\n",
    "model3 = create_densenet_model()\n",
    "history3, report3, y_pred3, _ = train_and_evaluate_model(model3, train_dataset, val_dataset, test_dataset)\n",
    "# history3, report3, y_pred3, _ = train_and_evaluate_model(model3, train_dataset, val_dataset, test_dataset)\n",
    "\n",
    "# 모델 4 (ResNet)\n",
    "model4 = create_resnet_model()\n",
    "history4, report4, y_pred4, _ = train_and_evaluate_model(model4, train_dataset, val_dataset, test_dataset)\n",
    "# history4, report4, y_pred4, _ = train_and_evaluate_model(model4, augmented_train_images, val_dataset, test_dataset)\n",
    "\n",
    "# 모델 5 (inception)\n",
    "model5 = create_resnet_model()\n",
    "history5, report5, y_pred5, _ = train_and_evaluate_model(model5, train_dataset, val_dataset, test_dataset)\n",
    "# history5, report5, y_pred5, _ = train_and_evaluate_model(model5, augmented_train_images, val_dataset, test_dataset)\n",
    "\n",
    "\n",
    "# 앙상블 예측\n",
    "y_pred_ensemble = (y_pred1 + y_pred2 + y_pred3 + y_pred4 + y_pred5) / 5\n",
    "y_pred_classes_ensemble = np.argmax(y_pred_ensemble, axis=1)\n",
    "\n",
    "# 앙상블 분류 보고서 생성\n",
    "report_ensemble = classification_report(y_true_classes, y_pred_classes_ensemble, target_names=class_names, output_dict=True)\n",
    "accuracy_ensemble = report_ensemble['accuracy']\n",
    "recall_ensemble = report_ensemble['weighted avg']['recall']\n",
    "precision_ensemble = report_ensemble['weighted avg']['precision']\n",
    "f1_score_ensemble = report_ensemble['weighted avg']['f1-score']\n",
    "\n",
    "print(f\"Ensemble Accuracy: {accuracy_ensemble}\")\n",
    "print(f\"Ensemble Recall: {recall_ensemble}\")\n",
    "print(f\"Ensemble Precision: {precision_ensemble}\")\n",
    "print(f\"Ensemble F1-score: {f1_score_ensemble}\")\n",
    "\n",
    "# ECE와 OE 계산\n",
    "ece_ensemble = calculate_ece(y_true_classes, y_pred_ensemble)\n",
    "oe_ensemble = calculate_oe(y_true_classes, y_pred_ensemble)\n",
    "\n",
    "print(f\"Ensemble ECE: {ece_ensemble}\")\n",
    "print(f\"Ensemble OE: {oe_ensemble}\")\n",
    "\n",
    "# 학습 과정 시각화\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history1.history['loss'], label='Baseline1 Train Loss')\n",
    "plt.plot(history1.history['val_loss'], label='Baseline1 Valid Loss')\n",
    "plt.plot(history2.history['loss'], label='VGG-like Train Loss')\n",
    "plt.plot(history2.history['val_loss'], label='VGG-like Valid Loss')\n",
    "plt.plot(history3.history['loss'], label='DenseNet-like Train Loss')\n",
    "plt.plot(history3.history['val_loss'], label='DenseNet-like Valid Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history1.history['accuracy'], label='Baseline1 Train Accuracy')\n",
    "plt.plot(history1.history['val_accuracy'], label='Baseline1 Valid Accuracy')\n",
    "plt.plot(history2.history['accuracy'], label='VGG-like Train Accuracy')\n",
    "plt.plot(history2.history['val_accuracy'], label='VGG-like Valid Accuracy')\n",
    "plt.plot(history3.history['accuracy'], label='DenseNet-like Train Accuracy')\n",
    "plt.plot(history3.history['val_accuracy'], label='DenseNet-like Valid Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
