{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/Marcus-Son/Classification_and_Calibration_of_Dysarthric_Speech/blob/yaejoon/%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%A0%84%EC%B2%98%EB%A6%AC_yaejoon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","source":["!pip install pydub"],"metadata":{"id":"Nxx_sly2QQPQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8SWEAXOIS38"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import librosa\n","import matplotlib.pyplot as plt\n","from pydub import AudioSegment\n","import torch\n","import pkg_resources\n","\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"Numpy version:\", np.__version__)\n","print(\"Pandas version:\", pd.__version__)\n","print(\"Librosa version:\", librosa.__version__)\n","print(\"Matplotlib version:\", plt.matplotlib.__version__)\n","print(\"PyTorch version:\", torch.__version__)\n","\n","# pydub version 확인\n","pydub_version = pkg_resources.get_distribution(\"pydub\").version\n","print(\"PyDub version:\", pydub_version)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsXaisMcRwLw"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from pydub.utils import db_to_float\n","import itertools\n","from pydub import AudioSegment\n","import os\n","\n","import IPython.display as ipd\n","from pydub import AudioSegment\n","import torch\n","import librosa # 음성데이터 분석 라이브러리\n","from IPython.display import Audio # 음성데이터 재생을 위해 사용하는 라이브러리"]},{"cell_type":"markdown","metadata":{"id":"8b3OjG6vRzBx"},"source":["# 1. 데이터 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81pKCDh7RwO2"},"outputs":[],"source":["# wav_folder = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/원천데이터/\"\n","# label_folder = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/라벨링데이터/\"\n","# result_folder = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/결과/\"\n","\n","# wav_folder_language = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/원천데이터/언어+뇌신경장애\"\n","# label_folder_language = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/라벨링데이터/언어+뇌신경장애\"\n","# result_folder_language = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/결과/언어+뇌신경장애\"\n","\n","# wav_folder_listen = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/원천데이터/청각+뇌신경장애\"\n","# label_folder_listen = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/라벨링데이터/청각+뇌신경장애\"\n","# result_folder_listen = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/결과/청각+뇌신경장애\"\n","\n","wav_folder = \"/content/gdrive/My Drive/data/원천데이터\"\n","label_folder = \"/content/gdrive/My Drive/data/라벨링데이터\"\n","result_folder = \"/content/gdrive/My Drive/data/결과\"\n","\n","wav_folder_language = \"/content/gdrive/My Drive/data/원천데이터/언어+뇌신경장애\"\n","label_folder_language = \"/content/gdrive/My Drive/data/라벨링데이터/언어+뇌신경장애\"\n","result_folder_language = \"/content/gdrive/My Drive/data/결과/언어+뇌신경장애\"\n","\n","wav_folder_listen = \"/content/gdrive/My Drive/data/원천데이터/청각+뇌신경장애\"\n","label_folder_listen = \"/content/gdrive/My Drive/data/라벨링데이터/청각+뇌신경장애\"\n","result_folder_listen = \"/content/gdrive/My Drive/data/결과/청각+뇌신경장애\""]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"FpS_-rEzSOes"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Np8jtTbOSKVs"},"source":["# 2. 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"Hk8pRsFGSgT7"},"source":["## 2-1. 함수 생성"]},{"cell_type":"markdown","source":["code reference : \"[음성 인식 모델 프로젝트] 음성 데이터 침묵구간 - 비침묵구간 분리하기,\" RecCode, last modified Jan 28, 2024, accessed May 19, 2024, https://ysg2997.tistory.com/52."],"metadata":{"id":"hMwv_lIL4m1S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mgw6jOoKR1sG"},"outputs":[],"source":["# detect_silence를 통해 침묵 구간 탐색\n","def detect_silence(audio_segment, min_silence_len=1000, silence_thresh=-16, seek_step=1, silence_has_gap=True):\n","    seg_len = len(audio_segment)\n","\n","    if seg_len < min_silence_len:\n","        return []\n","\n","    silence_thresh = db_to_float(silence_thresh) * audio_segment.max_possible_amplitude\n","\n","    silence_starts = []\n","\n","    last_slice_start = seg_len - min_silence_len\n","    slice_starts = range(0, last_slice_start + 1, seek_step)\n","\n","    if last_slice_start % seek_step:\n","        slice_starts = itertools.chain(slice_starts, [last_slice_start])\n","\n","    for i in slice_starts:\n","        audio_slice = audio_segment[i:i + min_silence_len]\n","        if audio_slice.rms <= silence_thresh:\n","            silence_starts.append(i)\n","\n","    if not silence_starts:\n","        return []\n","\n","    silent_ranges = []\n","\n","    prev_i = silence_starts.pop(0)\n","    current_range_start = prev_i\n","\n","    for silence_start_i in silence_starts:\n","        continuous = (silence_start_i == prev_i + seek_step)\n","\n","        if not continuous and silence_has_gap:\n","            silent_ranges.append([current_range_start,\n","                                  prev_i + min_silence_len])\n","            current_range_start = silence_start_i\n","        prev_i = silence_start_i\n","\n","    silent_ranges.append([current_range_start,\n","                          prev_i + min_silence_len])\n","\n","    return silent_ranges"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdKgbwJZR1ub"},"outputs":[],"source":["# detect_nonsilent함수를 통해 발화 구간 탐색\n","def detect_nonsilent(audio_segment, min_silence_len=1000, silence_thresh=-16, seek_step=1):\n","    silent_ranges = detect_silence(audio_segment, min_silence_len, silence_thresh, seek_step)\n","    len_seg = len(audio_segment)\n","\n","    if not silent_ranges:\n","        return [[0, len_seg]]\n","\n","    if silent_ranges[0][0] == 0 and silent_ranges[0][1] == len_seg:\n","        return []\n","\n","    prev_end_i = 0\n","    nonsilent_ranges = []\n","    for start_i, end_i in silent_ranges:\n","        nonsilent_ranges.append([prev_end_i, start_i])\n","        prev_end_i = end_i\n","\n","    if end_i != len_seg:\n","        nonsilent_ranges.append([prev_end_i, len_seg])\n","\n","    if nonsilent_ranges[0] == [0, 0]:\n","        nonsilent_ranges.pop(0)\n","\n","    return nonsilent_ranges"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2nJyJH7R1xB"},"outputs":[],"source":["# 위 두 함수를 통해 음성 데이터에서 최종 발화 구간과 침묵 구간 분리하여 반환\n","def create_json(audio_file):\n","  intervals_jsons = []\n","\n","  min_silence_length = 70\n","  intervals = detect_nonsilent(audio_file,\n","                               min_silence_len=min_silence_length,\n","                               silence_thresh=-32.64)\n","\n","  if intervals[0][0] != 0:\n","    intervals_jsons.append({'start':0,'end':intervals[0][0]/1000,'tag':'침묵'})\n","\n","  non_silence_start = intervals[0][0]\n","  before_silence_start = intervals[0][1]\n","\n","  for interval in intervals:\n","    interval_audio = audio_file[interval[0]:interval[1]]\n","\n","    if (interval[0] - before_silence_start) >= 20000:\n","      intervals_jsons.append({'start':non_silence_start/1000,'end':(before_silence_start+200)/1000,'tag':'비침묵'})\n","      non_silence_start = interval[0]-200\n","      intervals_jsons.append({'start':before_silence_start/1000,'end':interval[0]/1000,'tag':'침묵'})\n","    before_silence_start = interval[1]\n","\n","  if non_silence_start != len(audio_file):\n","    intervals_jsons.append({'start':non_silence_start/1000,'end':len(audio_file)/1000,'tag':'비침묵'})\n","\n","  return intervals_jsons\n","\n","\n","#########################################################\n","def match_target_amplitude(sound, target_dBFS):\n","\tchange_in_dBFS = target_dBFS - sound.dBFS\n","\treturn sound.apply_gain(change_in_dBFS)\n","\n","\n","#########################################################\n","def split_on_silence(audio_segment, min_silence_len=1000, silence_thresh=-16, keep_silence=100,\n","                     seek_step=1):\n","\n","    def pairwise(iterable):\n","        \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n","        a, b = itertools.tee(iterable)\n","        next(b, None)\n","        return zip(a, b)\n","\n","    if isinstance(keep_silence, bool):\n","        keep_silence = len(audio_segment) if keep_silence else 0\n","\n","    output_ranges = [\n","        [ start - keep_silence, end + keep_silence ]\n","        for (start,end)\n","            in detect_nonsilent(audio_segment, min_silence_len, silence_thresh, seek_step)\n","    ]\n","\n","    for range_i, range_ii in pairwise(output_ranges):\n","        last_end = range_i[1]\n","        next_start = range_ii[0]\n","        if next_start < last_end:\n","            range_i[1] = (last_end+next_start)//2\n","            range_ii[0] = range_i[1]\n","\n","    return [\n","        audio_segment[ max(start,0) : min(end,len(audio_segment)) ]\n","        for start,end in output_ranges\n","    ]"]},{"cell_type":"markdown","metadata":{"id":"7dIcf4pdS6hz"},"source":["## 2-2. 전처리 자동화 (wav, txt파일 생성)"]},{"cell_type":"markdown","source":["code reference : \"[음성 인식 모델 프로젝트] 음성 데이터 침묵구간 - 비침묵구간 분리하기,\" RecCode, last modified Jan 28, 2024, accessed May 19, 2024, https://ysg2997.tistory.com/52."],"metadata":{"id":"dlVEgR__5OB-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4oCCyxjBR1y1"},"outputs":[],"source":["# 폴더 변수화\n","PATH = os.getcwd()\n","\n","LABEL = os.path.join(PATH, label_folder)\n","AUDIO = os.path.join(PATH, wav_folder)\n","\n","LABEL_language = os.path.join(PATH, label_folder_language)\n","AUDIO_language = os.path.join(PATH, wav_folder_language)\n","\n","LABEL_listen = os.path.join(PATH, label_folder_listen)\n","AUDIO_listen = os.path.join(PATH, wav_folder_listen)\n","\n","# 결과 폴더는 직접 만들기\n","OUTPUT = os.path.join(PATH, result_folder)\n","OUTPUT_language = os.path.join(PATH, result_folder_language)\n","OUTPUT_listen = os.path.join(PATH, result_folder_listen)\n","\n","OUTPUT_language_spec = os.path.join(OUTPUT_language, \"spectrogram\")\n","OUTPUT_listen_spec = os.path.join(OUTPUT_listen, \"spectrogram\")"]},{"cell_type":"code","source":["OUTPUT_language_spec"],"metadata":{"id":"zSsUfmt9Wk8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaXoRfV1R107"},"outputs":[],"source":["print(\"라벨 폴더 폴더 개수: \", len(os.listdir(LABEL)))\n","print(\"음성 폴더 폴더 개수: \", len(os.listdir(AUDIO)))\n","print()\n","print(\"언어+뇌신경장애 라벨 폴더 파일 개수: \", len(os.listdir(LABEL_language)))\n","print(\"언어+뇌신경장애 음성 폴더 파일 개수: \", len(os.listdir(AUDIO_language)))\n","print()\n","print(\"청각+뇌신경장애 라벨 폴더 파일 개수: \", len(os.listdir(LABEL_listen)))\n","print(\"청각+뇌신경장애 음성 폴더 파일 개수: \", len(os.listdir(AUDIO_listen)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hce-q_HOFqDp"},"outputs":[],"source":["# import os\n","# import pandas as pd\n","# from pydub import AudioSegment\n","# from pydub.utils import make_chunks, mediainfo\n","# from scipy.io.wavfile import write\n","\n","# # 결과 폴더 생성\n","# for folder in [OUTPUT, OUTPUT_language, OUTPUT_listen]:\n","#     os.makedirs(folder, exist_ok=True)\n","\n","# # 라벨 및 오디오 폴더 설정 및 처리\n","# for folder_type, RESULT_FOLDER in [(\"language\", OUTPUT_language), (\"listen\", OUTPUT_listen)]:\n","#     if folder_type == \"language\":\n","#         LABEL_FOLDER = LABEL_language\n","#         AUDIO_FOLDER = AUDIO_language\n","#     elif folder_type == \"listen\":\n","#         LABEL_FOLDER = LABEL_listen\n","#         AUDIO_FOLDER = AUDIO_listen\n","\n","#     # 라벨 파일과 오디오 파일 리스트\n","#     label_files = sorted(os.listdir(LABEL_FOLDER))\n","#     audio_files = sorted(os.listdir(AUDIO_FOLDER))\n","\n","#     # 공통된 파일 개수만 처리\n","#     num_files = min(len(label_files), len(audio_files))\n","\n","#     for i in range(num_files):\n","#         # 스크립트 로드 (메타데이터, DataFrame)\n","#         print(f'파일: {i}')\n","#         label_file_path = os.path.join(LABEL_FOLDER, label_files[i])\n","#         meta = pd.read_json(label_file_path, orient='columns')\n","#         sampling_rate = meta['Meta_info']['SamplingRate']\n","#         meta = pd.DataFrame(meta['Transcript'][0].split(\".\"))[:-1]\n","\n","#         # 오디오 로드\n","#         audio_file_path = os.path.join(AUDIO_FOLDER, audio_files[i])\n","\n","#         # 침묵 및 비침묵 분리\n","#         sound = AudioSegment.from_file(audio_file_path, \"wav\")\n","#         normalized_sound = match_target_amplitude(sound, -20.0)\n","#         json_data = create_json(normalized_sound)\n","\n","#         # 음성/텍스트 세그먼트 로드 (DataFrame)\n","#         audio_df = pd.DataFrame(json_data)\n","#         df = pd.concat([audio_df[audio_df['tag'] == '비침묵'].reset_index(drop=True).drop('tag', axis=1), meta], axis=1)\n","#         df.columns = ['start', 'end', 'text']\n","\n","#         # 세그먼트용 오디오 로드\n","#         audio = AudioSegment.from_file(audio_file_path)\n","\n","#         # 문장별 데이터 저장 -> Nan 값은 제외\n","#         df_without_nan = df.dropna()\n","\n","#         for j, row in df_without_nan.iterrows():\n","#             start_time_sec = row['start']  # 시작 시간(초 단위)\n","#             end_time_sec = row['end']  # 종료 시간(초 단위)\n","\n","#             # 시작 시간과 종료 시간을 밀리초 단위로 변환\n","#             start_time_ms = start_time_sec * 1000\n","#             end_time_ms = (end_time_sec + 1.5) * 1000\n","\n","#             # 오디오 세그먼트 추출\n","#             output = audio[int(start_time_ms):int(end_time_ms)]\n","\n","#             # 세그먼트 길이 확인\n","#             clip_length = len(output) / 1000  # 밀리초에서 초로 변환\n","#             if clip_length > 12:\n","#                 continue  # 12초를 초과하는 세그먼트는 저장하지 않음\n","\n","#             # 세그먼트 오디오 파일 저장\n","#             output.export(os.path.join(RESULT_FOLDER, f\"output_{folder_type}{i}_{j}.wav\"), format=\"wav\")\n","\n","#             # 텍스트 파일 저장\n","#             text = row['text']\n","#             with open(os.path.join(RESULT_FOLDER, f\"output_{folder_type}{i}_{j}.txt\"), 'w') as f:\n","#                 f.write(text)\n","#                 f.write(\"\\n\")\n","#                 f.write(str(sampling_rate))\n","\n","# 결과 확인\n","print(\"언어 결과 파일:\", os.listdir(OUTPUT_language))\n","print(\"청각 결과 파일:\", os.listdir(OUTPUT_listen))"]},{"cell_type":"markdown","metadata":{"id":"NItLxxFZasPU"},"source":["## 2-3. 이미지로 전처리"]},{"cell_type":"code","source":["import os\n","import librosa\n","import librosa.display\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import cv2 as cv\n","\n","# 로그 멜 스펙트로그램 생성 함수\n","def compute_log_mel_spectrogram(signal, sample_rate, n_mels=128):\n","    frame_length = int(0.02 * sample_rate)  # 20ms를 샘플링레이트로 변환\n","    hop_length = int(frame_length / 2)      # 겹치는 부분\n","    stft = librosa.stft(signal, n_fft=frame_length, hop_length=hop_length)\n","    mel_spec = librosa.feature.melspectrogram(S=np.abs(stft)**2, sr=sample_rate, n_mels=n_mels)\n","    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n","    return log_mel_spec\n","\n","\n","def pad_to_square(spectrogram, target_size=128, padding_value=-80):\n","    if spectrogram.shape[1] < target_size:\n","        pad_width = target_size - spectrogram.shape[1]\n","        spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_width)), mode='constant', constant_values=padding_value)\n","    return spectrogram\n","\n","def split_and_save_spectrogram(log_mel_spec, base_filename, target_size=128, output_folder='./', sampling_rate=48000):\n","    \"\"\"Splits a spectrogram into smaller square segments and saves each segment as an image file.\"\"\"\n","    # Ensure the input spectrogram height matches the target size\n","    assert log_mel_spec.shape[0] == target_size, f\"Spectrogram height should be {target_size}\"\n","\n","    num_segments_x = log_mel_spec.shape[1] // target_size\n","\n","    for j in range(num_segments_x + 1):\n","        segment = log_mel_spec[:, j*target_size:(j+1)*target_size]\n","        if segment.shape[1] < target_size:\n","            segment = pad_to_square(segment, target_size)\n","        #print(segment.shape)\n","        segment_filename = os.path.join(output_folder, f\"{base_filename}_{j}.png\")\n","        save_spectrogram(segment, segment_filename)\n","\n","\n","# 스펙트로그램 저장 함수\n","def save_spectrogram(spectrogram, filename, sampling_rate=48000):\n","    plt.figure(figsize=(1.28, 1.28))\n","    plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0, hspace=0)  # 여백 조정\n","    librosa.display.specshow(spectrogram, x_axis='time', y_axis='mel', sr=sampling_rate, cmap='gray')\n","    plt.axis('off')\n","    plt.savefig(filename, bbox_inches='tight', pad_inches=0, dpi=100)\n","    plt.close()\n","\n","# 데이터프레임 생성을 위한 리스트 초기화\n","data = []\n","\n","def process_wav_files(folder_path, label, output_folder, target_size=128):\n","    for file_name in os.listdir(folder_path):\n","        if file_name.endswith(\".wav\"):\n","            file_path = os.path.join(folder_path, file_name)\n","            txt_path = file_path.replace('.wav', '.txt')\n","\n","            if not os.path.exists(txt_path):\n","                print(f\"Warning: {txt_path} does not exist.\")\n","                continue\n","\n","            with open(txt_path, 'r') as f:\n","                lines = f.readlines()\n","                sample_rate = int(lines[1].strip())\n","\n","            signal, sr = librosa.load(file_path, sr=sample_rate)\n","            signal, _ = librosa.effects.trim(signal, top_db=60, frame_length=int(0.02*sample_rate), hop_length=int(0.02*sample_rate))\n","            log_mel_spec = compute_log_mel_spectrogram(signal, sr)\n","            base_filename = os.path.splitext(file_name)[0]\n","            split_and_save_spectrogram(log_mel_spec, base_filename, target_size, output_folder, sampling_rate=sr)\n","            data.append({'file_name': file_name, 'log_mel_spec': log_mel_spec, 'label': label})"],"metadata":{"id":"3bl_htB_W7TT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OUTPUT_language_spec"],"metadata":{"id":"YTrT6jAQWQ1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 폴더 내의 WAV 파일 처리\n","# process_wav_files(OUTPUT_language, 0, OUTPUT_language_spec)"],"metadata":{"id":"j8hRJ00QPRxd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data[0]['log_mel_spec']"],"metadata":{"id":"EH5MJNmWlo4Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# process_wav_files(OUTPUT_listen, 1, OUTPUT_listen_spec)\n","\n","# '''\n","# # 데이터프레임 생성\n","# df = pd.DataFrame(data)\n","\n","# # 데이터프레임 확인\n","# print(df.head())\n","# '''"],"metadata":{"id":"b1EqpiuOXDyN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M1qlH2_rNJET"},"source":["# 3. CNN 모델링 테스트 ( 간이 실험 )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaJ3iKXVIS4G"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","\n","import os\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"n9aAEZUTqzru"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","\n","# 이미지 데이터가 저장된 경로\n","# data_dir = '/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/결과'\n","data_dir = \"/content/gdrive/My Drive/data/결과\"\n","\n","# 데이터셋 생성\n","train_dataset = image_dataset_from_directory(\n","    data_dir,\n","    labels='inferred',  # 폴더 이름을 라벨로 사용\n","    label_mode='categorical',   # 라벨을 정수형으로 설정\n","    color_mode='grayscale',  # 이미지는 흑백으로 로드\n","    batch_size=32,  # 배치 크기\n","    image_size=(28, 28),  # 이미지 크기 조정\n","    shuffle=True,  # 데이터 섞기\n","    validation_split=0.2,  # 검증 데이터 비율\n","    subset='training',  # 훈련 데이터셋으로 사용\n","    seed=123  # 랜덤 시드 설정\n",")\n","\n","validation_dataset = image_dataset_from_directory(\n","    data_dir,\n","    labels='inferred',\n","    label_mode='categorical',\n","    color_mode='grayscale',\n","    batch_size=32,\n","    image_size=(28, 28),\n","    shuffle=True,\n","    validation_split=0.2,\n","    subset='validation',\n","    seed=123\n",")\n","\n","class_names = train_dataset.class_names"],"metadata":{"id":"KE4MDa_Mqhcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","#class_names = train_dataset.class_names\n","# 데이터셋의 첫 번째 배치를 가져옵니다.\n","image_batch, label_batch = next(iter(train_dataset))\n","\n","# 배치에서 일부 이미지와 라벨을 시각화합니다.\n","plt.figure(figsize=(10, 10))\n","for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(image_batch[i].numpy().astype(\"uint8\"), cmap='gray')\n","    label = np.argmax(label_batch[i].numpy())  # 원-핫 인코딩된 라벨을 정수형으로 변환\n","    plt.title(\"language\" if (label==0) else \"listen\" if (label==1) else \"unknown\")\n","    plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"fDk_rrzWsJz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# 데이터셋 통합\n","full_dataset = train_dataset.concatenate(validation_dataset)\n","\n","# 모든 이미지와 라벨을 리스트로 변환\n","image_paths = []\n","labels = []\n","\n","for image, label in full_dataset.unbatch():\n","    image_paths.append(image.numpy())\n","    labels.append(label.numpy())\n","\n","# 리스트를 배열로 변환\n","image_paths = np.array(image_paths)\n","labels = np.array(labels)\n","\n","# 데이터셋을 60% 훈련, 20% 검증, 20% 테스트로 분리\n","X_temp, X_test, y_temp, y_test = train_test_split(image_paths, labels, test_size=0.2, random_state=123)\n","X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=123)\n","\n","# 배열을 tf.data.Dataset 객체로 변환\n","train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32).shuffle(buffer_size=1000)\n","validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32)\n","test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n","\n","# 데이터셋 확인\n","for images, labels in train_dataset.take(1):\n","    print(images.shape)\n","    print(labels.shape)"],"metadata":{"id":"s_B2KNxeYwR1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # 배열을 다시 텐서플로 데이터셋으로 변환\n","# import tensorflow as tf\n","\n","# train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32).shuffle(buffer_size=len(X_train))\n","# validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(32).shuffle(buffer_size=len(X_val))\n","# test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n","\n","# class_names = dataset.class_names"],"metadata":{"id":"aa60LAbmq8ug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, BatchNormalization, AveragePooling2D, Flatten, Dense\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras import optimizers\n","\n","# 모델 구성\n","model = Sequential([\n","    Conv2D(8, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),  # 흑백 이미지이므로 input_shape에서 채널 수를 1로 설정\n","    BatchNormalization(),\n","    AveragePooling2D(pool_size=(2, 2)),\n","\n","    Conv2D(16, kernel_size=(3, 3), activation='relu'),\n","    BatchNormalization(),\n","    AveragePooling2D(pool_size=(2, 2)),\n","\n","    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n","    BatchNormalization(),\n","    AveragePooling2D(pool_size=(2, 2)),\n","\n","    Flatten(),\n","    Dense(512, activation='relu'),\n","    Dense(128, activation='relu'),\n","    Dense(128, activation='relu'),\n","    Dense(2, activation='softmax')  # 클래스 수에 따라 출력 노드 수 조정\n","])\n","\n","model.summary()\n","\n","# 모델 컴파일\n","model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# 모델 학습\n","history = model.fit(\n","    train_dataset,\n","    validation_data=validation_dataset,\n","    epochs=50  # 원하는 에포크 수로 설정\n",")\n","\n","# 모델 평가\n","test_loss, test_accuracy = model.evaluate(test_dataset)\n","print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"],"metadata":{"id":"oFN9e0aCZekF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 결과 확인\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']"],"metadata":{"id":"69lyW7uErj5o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs_range = range(50)\n","\n","plt.figure(figsize=(16, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"],"metadata":{"id":"WJ8dZgd62lFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1WgTnJk2GHP8"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}