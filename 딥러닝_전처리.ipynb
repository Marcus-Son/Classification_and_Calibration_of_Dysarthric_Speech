{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWzMcoNCRrST"
      },
      "outputs": [],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pydub.utils import db_to_float\n",
        "import itertools\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "import IPython.display as ipd\n",
        "from pydub import AudioSegment\n",
        "import torch\n",
        "import librosa # 음성데이터 분석 라이브러리\n",
        "from IPython.display import Audio # 음성데이터 재생을 위해 사용하는 라이브러리"
      ],
      "metadata": {
        "id": "KsXaisMcRwLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 데이터 불러오기"
      ],
      "metadata": {
        "id": "8b3OjG6vRzBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "wav_folder = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/원천데이터/언어+뇌신경장애/\"\n",
        "label_folder = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/라벨링데이터/언어+뇌신경장애/\"\n",
        "result_folder = \"/content/gdrive/My Drive/deeplearning_2024/팀프로젝트/data/결과/\""
      ],
      "metadata": {
        "id": "81pKCDh7RwO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 데이터 전처리"
      ],
      "metadata": {
        "id": "Np8jtTbOSKVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1. 함수 생성 code reference : [음성 인식 모델 프로젝트] 음성 데이터 침묵구간 - 비침묵구간 분리하기," RecCode, last modified Jan 28, 2024, accessed May 19, 2024, https://ysg2997.tistory.com/52."
      ],
      "metadata": {
        "id": "Hk8pRsFGSgT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detect_silence를 통해 침묵 구간 탐색\n",
        "def detect_silence(audio_segment, min_silence_len=1000, silence_thresh=-16, seek_step=1, silence_has_gap=True):\n",
        "    seg_len = len(audio_segment)\n",
        "\n",
        "    if seg_len < min_silence_len:\n",
        "        return []\n",
        "\n",
        "    silence_thresh = db_to_float(silence_thresh) * audio_segment.max_possible_amplitude\n",
        "\n",
        "    silence_starts = []\n",
        "\n",
        "    last_slice_start = seg_len - min_silence_len\n",
        "    slice_starts = range(0, last_slice_start + 1, seek_step)\n",
        "\n",
        "    if last_slice_start % seek_step:\n",
        "        slice_starts = itertools.chain(slice_starts, [last_slice_start])\n",
        "\n",
        "    for i in slice_starts:\n",
        "        audio_slice = audio_segment[i:i + min_silence_len]\n",
        "        if audio_slice.rms <= silence_thresh:\n",
        "            silence_starts.append(i)\n",
        "\n",
        "    if not silence_starts:\n",
        "        return []\n",
        "\n",
        "    silent_ranges = []\n",
        "\n",
        "    prev_i = silence_starts.pop(0)\n",
        "    current_range_start = prev_i\n",
        "\n",
        "    for silence_start_i in silence_starts:\n",
        "        continuous = (silence_start_i == prev_i + seek_step)\n",
        "\n",
        "        if not continuous and silence_has_gap:\n",
        "            silent_ranges.append([current_range_start,\n",
        "                                  prev_i + min_silence_len])\n",
        "            current_range_start = silence_start_i\n",
        "        prev_i = silence_start_i\n",
        "\n",
        "    silent_ranges.append([current_range_start,\n",
        "                          prev_i + min_silence_len])\n",
        "\n",
        "    return silent_ranges"
      ],
      "metadata": {
        "id": "Mgw6jOoKR1sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detect_nonsilent함수를 통해 발화 구간 탐색\n",
        "def detect_nonsilent(audio_segment, min_silence_len=1000, silence_thresh=-16, seek_step=1):\n",
        "    silent_ranges = detect_silence(audio_segment, min_silence_len, silence_thresh, seek_step)\n",
        "    len_seg = len(audio_segment)\n",
        "\n",
        "    if not silent_ranges:\n",
        "        return [[0, len_seg]]\n",
        "\n",
        "    if silent_ranges[0][0] == 0 and silent_ranges[0][1] == len_seg:\n",
        "        return []\n",
        "\n",
        "    prev_end_i = 0\n",
        "    nonsilent_ranges = []\n",
        "    for start_i, end_i in silent_ranges:\n",
        "        nonsilent_ranges.append([prev_end_i, start_i])\n",
        "        prev_end_i = end_i\n",
        "\n",
        "    if end_i != len_seg:\n",
        "        nonsilent_ranges.append([prev_end_i, len_seg])\n",
        "\n",
        "    if nonsilent_ranges[0] == [0, 0]:\n",
        "        nonsilent_ranges.pop(0)\n",
        "\n",
        "    return nonsilent_ranges"
      ],
      "metadata": {
        "id": "vdKgbwJZR1ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위 두 함수를 통해 음성 데이터에서 최종 발화 구간과 침묵 구간 분리하여 반환\n",
        "def create_json(audio_file):\n",
        "  intervals_jsons = []\n",
        "\n",
        "  min_silence_length = 70\n",
        "  intervals = detect_nonsilent(audio_file,\n",
        "                               min_silence_len=min_silence_length,\n",
        "                               silence_thresh=-32.64)\n",
        "\n",
        "  if intervals[0][0] != 0:\n",
        "    intervals_jsons.append({'start':0,'end':intervals[0][0]/1000,'tag':'침묵'})\n",
        "\n",
        "  non_silence_start = intervals[0][0]\n",
        "  before_silence_start = intervals[0][1]\n",
        "\n",
        "  for interval in intervals:\n",
        "    interval_audio = audio_file[interval[0]:interval[1]]\n",
        "\n",
        "    if (interval[0] - before_silence_start) >= 20000:\n",
        "      intervals_jsons.append({'start':non_silence_start/1000,'end':(before_silence_start+200)/1000,'tag':'비침묵'})\n",
        "      non_silence_start = interval[0]-200\n",
        "      intervals_jsons.append({'start':before_silence_start/1000,'end':interval[0]/1000,'tag':'침묵'})\n",
        "    before_silence_start = interval[1]\n",
        "\n",
        "  if non_silence_start != len(audio_file):\n",
        "    intervals_jsons.append({'start':non_silence_start/1000,'end':len(audio_file)/1000,'tag':'비침묵'})\n",
        "\n",
        "  return intervals_jsons\n",
        "\n",
        "\n",
        "#########################################################\n",
        "def match_target_amplitude(sound, target_dBFS):\n",
        "\tchange_in_dBFS = target_dBFS - sound.dBFS\n",
        "\treturn sound.apply_gain(change_in_dBFS)\n",
        "\n",
        "\n",
        "#########################################################\n",
        "def split_on_silence(audio_segment, min_silence_len=1000, silence_thresh=-16, keep_silence=100,\n",
        "                     seek_step=1):\n",
        "\n",
        "    def pairwise(iterable):\n",
        "        \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
        "        a, b = itertools.tee(iterable)\n",
        "        next(b, None)\n",
        "        return zip(a, b)\n",
        "\n",
        "    if isinstance(keep_silence, bool):\n",
        "        keep_silence = len(audio_segment) if keep_silence else 0\n",
        "\n",
        "    output_ranges = [\n",
        "        [ start - keep_silence, end + keep_silence ]\n",
        "        for (start,end)\n",
        "            in detect_nonsilent(audio_segment, min_silence_len, silence_thresh, seek_step)\n",
        "    ]\n",
        "\n",
        "    for range_i, range_ii in pairwise(output_ranges):\n",
        "        last_end = range_i[1]\n",
        "        next_start = range_ii[0]\n",
        "        if next_start < last_end:\n",
        "            range_i[1] = (last_end+next_start)//2\n",
        "            range_ii[0] = range_i[1]\n",
        "\n",
        "    return [\n",
        "        audio_segment[ max(start,0) : min(end,len(audio_segment)) ]\n",
        "        for start,end in output_ranges\n",
        "    ]"
      ],
      "metadata": {
        "id": "z2nJyJH7R1xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. 전처리 자동화 (wav, txt파일 생성) code reference : [음성 인식 모델 프로젝트] 음성 데이터 침묵구간 - 비침묵구간 분리하기," RecCode, last modified Jan 28, 2024, accessed May 19, 2024, https://ysg2997.tistory.com/52."
      ],
      "metadata": {
        "id": "7dIcf4pdS6hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 폴더 변수화\n",
        "PATH = os.getcwd()\n",
        "\n",
        "LABEL = os.path.join(PATH, label_folder)\n",
        "AUDIO = os.path.join(PATH, wav_folder)\n",
        "\n",
        "# 결과 폴더는 직접 만들기\n",
        "OUTPUT = os.path.join(PATH, result_folder)"
      ],
      "metadata": {
        "id": "4oCCyxjBR1y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL"
      ],
      "metadata": {
        "id": "KuN1Mowy6bgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"라벨 폴더 파일 개수: \", len(os.listdir(LABEL)))\n",
        "print(\"음성 폴더 파일 개수: \", len(os.listdir(AUDIO)))"
      ],
      "metadata": {
        "id": "EaXoRfV1R107"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub.utils import mediainfo\n",
        "\n",
        "for i in range(len(os.listdir(LABEL))):\n",
        "    # 스크립트 불러오기 (meta, DataFrame)\n",
        "    print(f'파일: {i}')\n",
        "    label_file = os.path.join(LABEL, sorted(os.listdir(LABEL))[i])\n",
        "    meta = pd.read_json(label_file, orient='columns')\n",
        "    transcript = pd.DataFrame(meta['Transcript'][0].split(\".\"))[:-1]\n",
        "    sampling_rate = int(meta['Meta_info']['SamplingRate'])\n",
        "\n",
        "    # 오디오 불러오기\n",
        "    audio_file = os.path.join(AUDIO, sorted(os.listdir(AUDIO))[i])\n",
        "\n",
        "    # 침묵-비침묵 분리\n",
        "    sound = AudioSegment.from_file(audio_file, \"wav\")\n",
        "    normalized_sound = match_target_amplitude(sound, -20.0)\n",
        "    json = create_json(normalized_sound)\n",
        "\n",
        "    # 발화구간/텍스트/샘플레이트 불러오기 (df, DataFrame)\n",
        "    audio_df = pd.DataFrame(json)\n",
        "    df = pd.concat([audio_df[audio_df['tag'] == '비침묵'].reset_index(drop=True).drop('tag', axis=1), transcript], axis=1)\n",
        "    df.columns = ['start', 'end', 'text']\n",
        "\n",
        "    # 자를 오디오 불러오기\n",
        "    audio = AudioSegment.from_file(audio_file)\n",
        "\n",
        "    # 한 문장 별로 데이터 저장 -> 오류 나는 Nan값들 제외\n",
        "    df_without_nan = df.dropna()\n",
        "\n",
        "    for j in range(len(df_without_nan)):\n",
        "        start_time = int(df_without_nan['start'][j]) * 1000  # milliseconds\n",
        "        end_time = (int(df_without_nan['end'][j]) + 1.5) * 1000  # milliseconds\n",
        "\n",
        "        # 오디오 파일 자르기\n",
        "        output = audio[start_time:end_time]\n",
        "\n",
        "        # 오디오 클립의 길이 확인\n",
        "        clip_length = len(output) / 1000  # milliseconds to seconds\n",
        "        if clip_length > 12:\n",
        "            continue  # 12초를 초과하는 클립은 저장하지 않음\n",
        "\n",
        "        # 자른 오디오 파일 저장\n",
        "        output.export(os.path.join(OUTPUT, f\"output{i}_{j}.wav\"), format=\"wav\")\n",
        "\n",
        "        # 텍스트 파일 저장\n",
        "        text = df_without_nan['text'][j]\n",
        "        with open(os.path.join(OUTPUT, f\"output{i}_{j}.txt\"), 'w') as f:\n",
        "            f.write(str(text+\"\\n\"+str(sampling_rate)))\n",
        "\n",
        "    # 결과 확인\n",
        "    os.listdir(OUTPUT)"
      ],
      "metadata": {
        "id": "37cRq3ElR13A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "2KUZ3WTfGLH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-3. 이미지로 전처리"
      ],
      "metadata": {
        "id": "NItLxxFZasPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제점 전처리가 제대로 되지 않은 파일들도 존재\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "# 폴더 내 모든 WAV 파일 읽어오기\n",
        "file_paths = [os.path.join(OUTPUT, f) for f in os.listdir(OUTPUT) if f.endswith('.wav')]\n",
        "\n",
        "# 각 WAV 파일에 대해 그래프 그리기\n",
        "for file_path in file_paths:\n",
        "    data, sample_rate = librosa.load(file_path, sr=48000)\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    librosa.display.waveshow(y=data, sr=sample_rate)\n",
        "    plt.title(os.path.basename(file_path))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nPvmM4rvaxXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 함수 정의\n",
        "def compute_log_mel_spectrogram(signal, sample_rate, n_mels=128):\n",
        "    # 1. 음성 신호를 프레임으로 분할\n",
        "    frame_length = int(0.02 * sample_rate)  # 20ms를 샘플링레이트로 변환\n",
        "    hop_length = int(frame_length / 2)      # 겹치는 부분\n",
        "    # 2. 각 프레임에 대한 FFT 수행\n",
        "    stft = librosa.stft(signal, n_fft=frame_length, hop_length=hop_length)\n",
        "    # 3. 주파수-시간 영역을 멜 스케일로 변환\n",
        "    mel_spec = librosa.feature.melspectrogram(S=np.abs(stft)**2, sr=sample_rate, n_mels=n_mels)\n",
        "    # 4. 로그 변환\n",
        "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    return log_mel_spec\n",
        "\n",
        "# 결과 플롯을 위한 함수\n",
        "def plot_log_mel_spectrogram(log_mel_spec, sample_rate, title):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(log_mel_spec, sr=sample_rate, x_axis='time', y_axis='mel')\n",
        "    plt.title(title)\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 디렉토리 내의 모든 output*.wav 파일에 대해 처리\n",
        "for file_name in os.listdir(OUTPUT):\n",
        "    if file_name.startswith(\"output\") and file_name.endswith(\".wav\"):\n",
        "        file_path = os.path.join(OUTPUT, file_name)\n",
        "        signal, _ = librosa.load(file_path, sr=sample_rate)\n",
        "        log_mel_spec = compute_log_mel_spectrogram(signal, sample_rate)\n",
        "        plot_log_mel_spectrogram(log_mel_spec, sample_rate, f'Log Mel Spectrogram: {file_name}')"
      ],
      "metadata": {
        "id": "Odwg2UDPaxec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_mel_spec"
      ],
      "metadata": {
        "id": "lJhZ25Gv0RVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_log_mel_spectrogram(signal, sample_rate, n_mels=128):\n",
        "    # 1. 음성 신호를 프레임으로 분할\n",
        "    frame_length = int(0.02 * sample_rate)  # 20ms를 샘플링레이트로 변환\n",
        "    hop_length = int(frame_length / 2)      # 겹치는 부분\n",
        "    # 2. 각 프레임에 대한 FFT 수행\n",
        "    stft = librosa.stft(signal, n_fft=frame_length, hop_length=hop_length)\n",
        "    # 3. 주파수-시간 영역을 멜 스케일로 변환\n",
        "    mel_spec = librosa.feature.melspectrogram(S=np.abs(stft)**2, sr=sample_rate, n_mels=n_mels)\n",
        "    # 4. 로그 변환\n",
        "    log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    return log_mel_spec\n"
      ],
      "metadata": {
        "id": "3gNrSmlMf4Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "#mfcc=librosa.feature.mfcc(y=y, sr=sr)\n",
        "def compute_and_save_mfcc(directory, output_dir, sampling_rate):\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Find all .wav files\n",
        "    file_paths = glob(os.path.join(directory, 'output*.wav'))\n",
        "\n",
        "    # Process each file\n",
        "    for file_path in file_paths:\n",
        "        # Load the audio file\n",
        "        y, sr = librosa.load(file_path, sr=sampling_rate)\n",
        "\n",
        "        # MFCC\n",
        "        # n_fft = window_length = sampling_rate * 25ms\n",
        "        # hop_length = sampling_rate * 20ms\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_fft=1200, hop_length=960)\n",
        "\n",
        "        # Create a file name\n",
        "        base_name = os.path.basename(file_path)\n",
        "        npy_file_name = base_name.replace('.wav', '.npy')\n",
        "        npy_file_path = os.path.join(output_dir, npy_file_name)\n",
        "\n",
        "        # Save the MFCC to numpy\n",
        "        np.save(npy_file_path, mfcc)\n",
        "        print(f\"Saved MFCC to {npy_file_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TyWACudKpG99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_and_save_mfcc(OUTPUT, OUTPUT, sampling_rate)"
      ],
      "metadata": {
        "id": "ob4ss6n6Sli6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfcc = np.load(OUTPUT+'output0_0.npy')\n",
        "\n",
        "hop_length=960\n",
        "n_fft=1200\n",
        "sampling_rate=48000\n",
        "time_axis = np.arange(mfcc.shape[1]) * hop_length / sampling_rate\n",
        "\n",
        "# Plot the MFCC data\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.imshow(mfcc, aspect='auto', origin='lower', cmap='viridis',extent=[0, max(time_axis), 0, mfcc.shape[0]])\n",
        "\n",
        "# Adding labels and title\n",
        "plt.title('MFCC')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('MFCC Coefficients')\n",
        "plt.colorbar(label='Amplitude')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rTaUmflXTbIJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
